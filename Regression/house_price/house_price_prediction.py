# -*- coding: utf-8 -*-
"""House_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hcuMQX6g9foDA_RN5zxJg4-XujFMVvIS

<h1>House Price</h1>
<p>In this notebook i conducted a brief data analysis and used multiple models then choosed the best one<p/>
<h3>Columns</h3>
<ul>
    <li>Longitude</li>
    <li>Latitude</li>
    <li>Housing Median Age</li>
    <li>Total rooms</li>
    <li>Total Bedrooms</li>
    <li>Population</li>
    <li>Households</li>
    <li>Median income</li>
    <li>Median House Value</li>
    <li>Ocean Proximity</li>
</ul>

<h6>Importing Libraries</h6>
"""

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
plt.style.use("ggplot")
pd.set_option("display.max_columns", 200)
from sklearn.model_selection import train_test_split


#models
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score

"""<h6>Reading the data into a dataframe</h6>"""

df = pd.read_csv("housing.csv")

df.head()

"""<h6>Checking the columns</h6>"""

df.columns

"""<h6>Checking the shape to know the number of rows and columns</h6>"""

df.shape

"""<h6>Getting basic info from the dataset to know the data types we have</h6>"""

df.info()

"""<h6>Getting the basic stats from the dataset</h6>"""

df.describe()

"""<h6>Looking for null values</h6>"""

df.isnull().sum()

"""<h6>Subsetting the "Ocean Proximity"</h6>
<p>Getting the unique values</p>
"""

df["ocean_proximity"].unique()

"""<h6>Checking for duplicates</h6>"""

df.duplicated().sum()

df.columns

"""<h6>Since all the columns are non numerical we take only the numerical columns for statistical analysis</h6>"""

numeric_cols = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',
       'total_bedrooms', 'population', 'households', 'median_income',
       'median_house_value']

"""<h2>Plotting a histogram to understand the distribution of all numerical colums</h2>"""

df[numeric_cols].hist(figsize=(12,10))
plt.suptitle("Histogram Of all the numeric columns", y=1.02)
plt.show()

"""<H6>BOX PLOTS</H6>"""

plt.figure(figsize=(12, 8))
sns.boxplot(data=df[numeric_cols])
plt.title('Box plots of Numeric Variables')
plt.xticks(rotation=45)
plt.show()

"""<h6>Scatter Plot is used to visualize relationship between data</h6>"""

plt.figure(figsize=(12, 8))
sns.pairplot(df[numeric_cols])
plt.suptitle('Pairwise Scatter plots of Numeric Variables', y=1.02)
plt.show()

"""<H4>Correlation Matrix</H4>"""

plt.figure(figsize=(10, 8))
corr_matrix = df[numeric_cols].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

"""<H6>Scatter plots of numeric features against the target variable</H6>"""

for col in numeric_cols:
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=col, y='median_house_value', data=df)
    plt.title(f'Scatter plot of {col} vs. Median House Value')
    plt.xlabel(col)
    plt.ylabel('Median House Value')
    plt.show()

"""<h4>Calculating correlation coefficients</h4>"""

correlation_coeffs = df[numeric_cols].corrwith(df['median_house_value'])

print("Correlation coefficients with 'median_house_value':")
print(correlation_coeffs)

df

"""<h2>Preprocessing the data</h2>"""

#splitting the dataset

X = df.drop("median_house_value", axis=1)
y = df["median_house_value"]

X

X["total_bedrooms"].isnull().sum()

"""<p>Here i will fill in the missing values with the mean value of the column, using the fillna() method</p>"""

X["total_bedrooms"] = X["total_bedrooms"].fillna(X["total_bedrooms"].mean())

X.isna().sum()

"""<h7>Using get dummies method in pandas to convert the non-numerical column into binary classes {0,1}</h7>"""

encoded_X = pd.get_dummies(X, columns=['ocean_proximity'])

encoded_X

"""<h6>building the model</h6>
<p>Splitting our data into training and testing data, as a rule of thumb i used 80% as training data and 20% as testing data, 75% and 25% can also be used</p>

<h3>The Goal is to build multiple models and select the best performers then proceed to perform dimensionality reduction and model validation</h3>

<h2>Support Vector Regressor</h2>
"""

#Converting the dependent and independent features to an array
encoded_X = encoded_X.values
y = y.values

# Splitting the dataset into train and test data


X_train, X_test, y_train, y_test = train_test_split(encoded_X, y,
                                                    test_size=0.2,
                                                    random_state=0)

# Support vector machine always take scaled features so i scalled the feature

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()

# Fit and transform X_train
X_train = sc_X.fit_transform(X_train)

# Only transform X_test (never fit on test data)
X_test = sc_X.transform(X_test)

# Fit and transform y_train
y_train = sc_y.fit_transform(y_train.reshape(-1, 1))

# Instantiatiating the model
regressor = SVR(kernel='rbf')
regressor.fit(X_train, y_train)

# Make predictions on the scaled test data and then inverse transform to get original scale

y_pred = regressor.predict(X_test)

# since the data was scaled we have transform it back
y_pred = sc_y.inverse_transform(y_pred.reshape(-1,1))

# scoring the model
from sklearn.metrics import r2_score

score_svr = r2_score(y_test,y_pred)
print(score_svr)

"""<H2>Multiple Linear Regression </h2>"""

# splitting the data into train and test sets


X_train, X_test, y_train, y_test = train_test_split(encoded_X,y,
                                                   test_size=0.2,
                                                   random_state=0)

# instantiating the model object
linear_regressor = LinearRegression()
#fitting the data into the model
linear_regressor.fit(X_train, y_train)

# predicting on the test data
y_pred_mult = linear_regressor.predict(X_test)

# scoring the model
score_multi_linear = r2_score(y_test,y_pred_mult)
print(score_multi_linear)

"""<h4>Polynomial Regression</h4>"""

# splitting the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(encoded_X,y,
                                                   test_size=0.2,
                                                   random_state=0)

# Training the polynomial Regression model on training set

# instantiating the model to a degree of 2
poly_reg = PolynomialFeatures(degree=2)

# transforming the data the data into a polynomial
X_poly = poly_reg.fit_transform(X_train)

#Instantiating the model
poly_regressor = LinearRegression()

#Fitting the model
poly_regressor.fit(X_poly, y_train)

#Predicting the test set
y_pred = poly_regressor.predict(poly_reg.transform(X_test))

# Scoring the model
score_poly = r2_score(y_test, y_pred)
print(score_poly)

"""<h6>Decision Tree</h6>"""

# testing the data
X_train, X_test, y_train, y_test = train_test_split(encoded_X,y,
                                                   test_size=0.2,
                                                   random_state=0)


# instantiating the model
decision_regressor = DecisionTreeRegressor(random_state=0)

#Fitting the model
decision_regressor.fit(X_train,y_train)

#Predicting the test set
y_pred = decision_regressor.predict(X_test)

#scoring the model
score_decision = r2_score(y_test,y_pred)
score_decision

"""<h6> Random Forest </h6>"""

# splitting the data into train and test data
X_train, X_test, y_train, y_test = train_test_split(encoded_X,y,
                                                   test_size=0.2,
                                                   random_state=0)

#Instantiating the model with 10 estimators
random_regressor = RandomForestRegressor(n_estimators=20,
                                        random_state=0)

# fitting the data
random_regressor.fit(X_train, y_train)

#predicting on the test set
y_pred = random_regressor.predict(X_test)

#scoring the model
score_randomForest = r2_score(y_test, y_pred)
print(score_randomForest)

"""<h3>XGBOOST</h3>"""

# splitting the data into train and test data
X_train, X_test, y_train, y_test = train_test_split(encoded_X,y,
                                                   test_size=0.2,
                                                   random_state=0)

#Instantiating the model
regressor = XGBRegressor()
#fitting the model
regressor.fit(X_train, y_train)

# predicting the test set
y_pred = regressor.predict(X_test)

# scoring the model
score_XGBOOST = r2_score(y_test,y_pred)
print(score_XGBOOST)

"""<h2>Comparing the model</h2>"""

models = pd.DataFrame({
    "Models": ["Support Vector Regression","Multi-Linear Regression", "Polynomial Regression",
              "DecisionTree Regression", "RandomForest Regressor", "XGBOOST"],
    "Score": [score_svr,score_multi_linear, score_poly, score_decision,score_randomForest,score_XGBOOST]
})

models.sort_values(by="Score",ascending=False)

"""<p>Let's visualize the the comparisons</p>"""

# Set figure size
plt.figure(figsize=(10, 6))

# Define custom color palette
colors = ["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b"]

# Create bar plot with custom palette and larger font size
sns.barplot(x=models['Models'], y=models['Score'], hue=models['Models'], palette=colors, dodge=False)

# Set plot title and labels
plt.title('Model Comparison')
plt.xlabel('Models')
plt.ylabel('Score')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show plot
plt.show()

"""<p>Now we can clearly see that the XGBOOST has the highest score followed by Randome forest. So now we will focus on improving these two models and choose the best out of them.</p>
<h2>What we will do</h2>
<ul>
<li>GridSearch Hyperparameter Tunning to improve the model</li>
<li>Model evaluation</li>
</ul>
"""

# Hyperparameter Tunning

X_train, X_test, y_train, y_test = train_test_split(encoded_X,y,
                                                   test_size=0.2,
                                                   random_state=0)

from sklearn.model_selection import GridSearchCV

# Define the RandomForestRegressor
rf = RandomForestRegressor()

# Define the hyperparameters grid to search over
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=rf,
                           param_grid=param_grid,
                           cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

# Print the best hyperparameters found
print("Best hyperparameters for randomforest:", grid_search.best_params_)

# Define the XGBoost regressor
xgb_reg = XGBRegressor()

# Define the hyperparameter grid to search over
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# Perform grid search with cross-validation
grid_search_x = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, scoring='r2')
grid_search_x.fit(X_train, y_train)

# Print the best hyperparameters found
print("Best hyperparameters for xgb:", grid_search_x.best_params_)

"""<h4>Working with the best scores</h4>"""

# splitting the data into train and test data
X_train, X_test, y_train, y_test = train_test_split(encoded_X,y,
                                                   test_size=0.2,
                                                   random_state=0)
best_randomF = RandomForestRegressor(n_estimators=400,
                                     max_depth=None,
                                     min_samples_split=2,
                                     min_samples_leaf=2)
# fitting the model with the best param
best_randomF.fit(X_train,y_train)

# predicting the model
y_pred = best_randomF.predict(X_test)

# scoring the model
best_score_randomF = r2_score(y_test,y_pred)
print(best_score_randomF)

# splitting the data into train and test data
X_train, X_test, y_train, y_test = train_test_split(encoded_X,y,
                                                   test_size=0.2,
                                                   random_state=0)

#Instantiating the model
best_regressor = XGBRegressor(colsample_bytree=0.8,
                              learning_rate=0.1,
                              max_depth=7,
                              min_child_weight=1,
                              subsample=0.9,
                              n_estimators=400
                              )
#fitting the model
best_regressor.fit(X_train, y_train)

# predicting the test set
y_pred = best_regressor.predict(X_test)

# scoring the model
best_score_XGBOOST = r2_score(y_test,y_pred)
print(best_score_XGBOOST)

"""<h3> Model Evaluation</h3>"""

rf_cross_val = cross_val_score(estimator = best_randomF, X = X_train, y = y_train, cv = 10, scoring="r2")
print("Score: {:.2f} %".format(rf_cross_val.mean()*100))
print("Standard Deviation: {:.2f} %".format(rf_cross_val.std()*100))

xgb_cross_val = cross_val_score(estimator = best_regressor, X = X_train, y = y_train, cv = 10, scoring="r2")
print("Score: {:.2f} %".format(xgb_cross_val.mean()*100))
print("Standard Deviation: {:.2f} %".format(xgb_cross_val.std()*100))