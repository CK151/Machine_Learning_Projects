# -*- coding: utf-8 -*-
"""Car_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l-gR5kPuiyfMXuQzD-ddp_g4bpUkGGrG

# Car Price Prediction
"""

# Importing libraries

import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score

"""### Loading the data"""

dataset = pd.read_csv("CarPrice_Assignment.csv")
dataset

dataset.shape # checking the shape of our dataset

"""<p>We can see that the dataset has 205 rows and 26 columns</p>"""

dataset.columns # checking the columns

"""### Familarity with the dataset"""

dataset.isna().sum() # checking for missing values

"""<p>The dataset has no missing values</p>"""

dataset.nunique() # checking for unique variables

"""The dataset has alot of unique values we can see that the car name has 147 unique values out of 205 columns, carbody has 5 unique values and the rest, this shows that we have a lot of investigation to perfom"""

dataset.info() # to get the basic information about our column

"""Here we can see that out of 26 columns we have 10 non-numerical columns"""

dataset.describe().T

"""### Descriptive Statistics

- Price: The average car price is 13276.71 and has a standard deviation of of 7988.85 with a minimal value of 5118 and a maximum value of 45400 we have 3rd quantile of 16593
"""

categorical_cols = ["fueltype","aspiration","doornumber","carbody", # getting the name of all categorical columns so as to know their index value
                    "drivewheel","enginelocation","enginetype","cylindernumber",
                    "fuelsystem",]


for column in categorical_cols: # looping through the categorical columns
  print(f"Categories in {column} is: {dataset[column].unique()}")

dataset.CarName.unique() # Unique values for carname

"""## Feature Extraction

Since the car name has so many different elements, then we have to extraxt the car brand an model because that is what we have in the car name column
"""

dataset['brand'] = dataset['CarName'].apply(lambda x: x.split(' ')[0]) # this is an annonymous fuction that extract the first index from the carname
dataset['model'] = dataset['CarName'].apply(lambda x: " ".join(x.split(' ')[1:])) # this function extracts every other thing except the first index

dataset["brand"] = dataset["brand"].replace({'vw':'volkswagen', "toyouta":'toyota', # here we are trying to replace misspelled or abbrevaited words
                                             'Nissan':'nissan', 'porcshce':'porsche', 'maxda':'mazda','vokswagen':'volkswagen'})

dataset["model"] = dataset["model"].replace({"100 ls": "100ls"})
dataset["brand"].unique(), dataset['model'].unique()

dataset.columns

"""Here we can see that we have the brand and model column added to the dataset, now you will agree with me that is safe to drop the carName and id column since they are irrelevant"""

data_cleaned = dataset.drop(["CarName", "car_ID"], axis=1)

"""### Data Visualisation"""

categorical_cols = ["fueltype","aspiration","doornumber","carbody",
                    "drivewheel","enginelocation","enginetype","cylindernumber",
                    "fuelsystem","model", "brand"]

for col in categorical_cols:
    plt.figure(figsize=(8,6))
    sns.countplot(x=col, data=data_cleaned)
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.title('Count Plot')
    plt.show()

"""We can see how the categorcal data are using the count plot, the data is not balanced, however we will not perform downsampling because out dataset is is low just 205 rows, you can upsample but for this project we will work with it like this"""

numerical_cols = ["symboling","wheelbase", "carlength", "carwidth", "carheight", "curbweight", "enginesize", "boreratio",
                  "stroke", "compressionratio", "horsepower", "peakrpm", "citympg", "highwaympg", "price"]

num_cols = len(numerical_cols)

num_rows = (num_cols + 3) // 4
# Set the style of the plots
sns.set(style="whitegrid")

# Create subplots for each numerical column
fig, axes = plt.subplots(nrows=num_rows, ncols=4, figsize=(14, 3 * num_rows))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Loop through the numerical columns and plot the distribution for each
for i, col in enumerate(numerical_cols):
    sns.histplot(data_cleaned[col], ax=axes[i], kde=True)
    axes[i].set_title(col)

# Hide extra subplots if any
for j in range(num_cols, num_rows * 4):
    axes[j].axis('off')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

"""We can see that the majoority of our data has normal distribution except for price, horsepower compresssion ratio an enginsize"""

f, (ax1, ax2, ax3,ax4) = plt.subplots(1, 4, sharey=True, figsize =(15,3))
ax1.scatter(data_cleaned['symboling'],data_cleaned['price'])
ax1.set_title('Price and Symboling')
ax2.scatter(data_cleaned['compressionratio'],data_cleaned['price'])
ax2.set_title('Price and compressionratio')
ax3.scatter(data_cleaned['horsepower'],data_cleaned['price'])
ax3.set_title('Price and horsepower')
ax4.scatter(data_cleaned['highwaympg'],data_cleaned['price'])
ax4.set_title('Price and highwaympg')

plt.show()

corr_matrix = data_cleaned[numerical_cols].corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show();

"""Here we can see that there is high correlation between most of the variables, now the idea of correlation is that, if two values are highly correlated that means that one value can be used to predict the other therefore no need keeping both values, we will further investigate this using Variance Inflation factor ðŸ‘®ðŸ¿â€â™€ï¸ðŸ‘¨ðŸ¿â€ðŸ’»"""

gas_fueltype = data_cleaned[data_cleaned.fueltype == "gas"]
diesel_fueltype = data_cleaned[data_cleaned.fueltype  == "diesel"]

len(gas_fueltype), len(diesel_fueltype)

gas_fueltype.describe().T

"""We can see that the average price for gas cars is $12999, with a min of 5118 and a max of 45000"""

diesel_fueltype.describe().T

"""The average for diesel cars is $15000 a bit higher than the gas cars"""

value_counts = data_cleaned['fueltype'].value_counts()

# Plotting the pie chart using Seaborn
plt.figure(figsize=(6, 6))  # Adjust the figure size if needed
plt.title('Distribution of Gas cars')
sns.set_palette('pastel')  # Set color palette
plt.pie(value_counts, labels=['Gas', 'Diesel'], autopct='%1.1f%%')
plt.xlabel('Fuel Type Status')
plt.show()

data_cleaned.drivewheel.value_counts()

"""Now we can clearly see that th 4wd was an abbreviation of fwd so we will replace it"""

data_cleaned["drivewheel"] = data_cleaned.drivewheel.replace({"4wd": "fwd"})

data_cleaned.drivewheel.value_counts()

data_cleaned[data_cleaned.drivewheel == "fwd"].describe().T

data_cleaned[data_cleaned.drivewheel == "rwd"].describe().T

value_counts = data_cleaned.drivewheel.value_counts()

# Plotting the pie chart using Seaborn
plt.figure(figsize=(6, 6))  # Adjust the figure size if needed
plt.title('Distribution of Drive Wheel')
sns.set_palette('pastel')  # Set color palette
plt.pie(value_counts, labels=['Fwd', 'rwd'], autopct='%1.1f%%')
plt.xlabel('Drive Wheel Status')
plt.show()

data_cleaned.doornumber.value_counts()

data_cleaned[data_cleaned.doornumber == "four"].describe().T

data_cleaned[data_cleaned.doornumber == "two"].describe().T

value_counts = data_cleaned['doornumber'].value_counts()

# Plotting the pie chart using Seaborn
plt.figure(figsize=(6, 6))  # Adjust the figure size if needed
plt.title('Distribution of Door Number')
sns.set_palette('pastel')  # Set color palette
plt.pie(value_counts, labels=['Four', 'Two'], autopct='%1.1f%%')
plt.xlabel('Door number type')
plt.show()

data_cleaned.carbody.value_counts()

value_counts = data_cleaned['carbody'].value_counts()

# Plotting the pie chart using Seaborn
plt.figure(figsize=(6, 6))  # Adjust the figure size if needed
plt.title('Distribution of Car Body')
sns.set_palette('pastel')  # Set color palette
plt.pie(value_counts, labels=['Sedan', 'Hatchback', "Wagon", "Hardtop","Covertible"], autopct='%1.1f%%')
plt.xlabel('Car Body')
plt.show()

data_cleaned.brand.value_counts()

value_counts = data_cleaned['brand'].value_counts()

# Plotting the pie chart using Seaborn
plt.figure(figsize=(12, 12))  # Adjust the figure size if needed
plt.title('Distribution of Car Brands')
sns.set_palette('pastel')  # Set color palette
plt.pie(value_counts, labels=['Toyota', 'Nissan', "Mazda", "Mitsubishi","Honda", "Volskwagen", "Saburu", "Peugeot", "Volvo", "Dodge", "Builk",
                              "BMW", "Audi", "Plymouth", "Saab", "Porsche", "Isuzu", "Jaguar", "Chevrolet", "Alfa-romero", "Renault", "Mercury"], autopct='%1.1f%%')
plt.xlabel('Car Brands')
plt.show()

"""We can see that Toyota is popular among users followed by Nissan and then Mazda

### Checking for multicollinearity
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
variables = data_cleaned[numerical_cols]
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif["Features"] = variables.columns

vif

"""ðŸ¥¶ðŸ˜± We can see that the VIF values for some of our variables are so high, a good vif is between 1 and 5 however some statistician recommend not more than 10. So for this project we will set it to >= 1 <= 10"""

numerical = ['symboling', # here we will remove all the variables that have a high vif value. Please know that this was achieved after many trials
 'compressionratio',
 'horsepower',
 'highwaympg']

variables = data_cleaned[numerical]
vif_1 = pd.DataFrame()
vif_1["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif_1["Features"] = variables.columns

vif_1

corr_matrix = data_cleaned[numerical].corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show();

"""ðŸ¥°ðŸ˜Ž This is very beautiful, we can see that our correlation matrix values has drastically reduced. So we will work with these variables"""

data_cleaned

data_cleaned.dtypes

model_columns = ['symboling',
                'compressionratio',
                'horsepower',
                  'highwaympg', "fueltype",'aspiration', 'doornumber', 'carbody','drivewheel','enginelocation',
                      'brand',"enginetype","cylindernumber","fuelsystem","price"]


first_model_data = data_cleaned[model_columns] # subsetting our dataset with the vif variables


first_model_data

"""### Data preprocessing"""

first_model_data.fuelsystem.value_counts()

first_model_data = first_model_data.rename(columns={"fueltype":"fueltype_gas"}) #renaming the column so that when we see 1 which means fuel system is gas

first_model_data.fueltype_gas = first_model_data.fueltype_gas.replace({"gas":1, "diesel":0})

first_model_data.fueltype_gas.value_counts()

first_model_data.doornumber = first_model_data.doornumber.replace({"two":2, "four":4})

first_model_data.doornumber.value_counts()

first_model_data = first_model_data.rename(columns={"aspiration":"aspiration_std"})


first_model_data.aspiration_std = first_model_data.aspiration_std.replace({"std":1, "turbo":0})

first_model_data.aspiration_std.value_counts()

first_model_data = first_model_data.rename(columns={"drivewheel":"drivewheel_fwd"})


first_model_data.drivewheel_fwd = first_model_data.drivewheel_fwd.replace({"fwd":1, "rwd":0})

first_model_data.drivewheel_fwd.value_counts()

first_model_data = first_model_data.rename(columns={"drivewheel":"drivewheel_fwd"})


first_model_data.drivewheel_fwd = first_model_data.drivewheel_fwd.replace({"fwd":1, "rwd":0})

first_model_data.drivewheel_fwd.value_counts()

"""Here we have completed numerical conversion of  the binary categorical variables"""

first_model_data.dtypes

data_with_dummies = pd.get_dummies(first_model_data, drop_first=True) # drop first making sure we have n-1 for the dummies

data_with_dummies

first_model_data.shape, data_with_dummies.shape

"""#### Rearranging the columns a bit"""

cols =['symboling', 'compressionratio', 'horsepower', 'highwaympg',
       'fueltype_gas', 'aspiration_std', 'doornumber', 'drivewheel_fwd', 'carbody_hardtop', 'carbody_hatchback', 'carbody_sedan',
       'carbody_wagon', 'enginelocation_rear', 'brand_audi', 'brand_bmw',
       'brand_buick', 'brand_chevrolet', 'brand_dodge', 'brand_honda',
       'brand_isuzu', 'brand_jaguar', 'brand_mazda', 'brand_mercury',
       'brand_mitsubishi', 'brand_nissan', 'brand_peugeot', 'brand_plymouth',
       'brand_porsche', 'brand_renault', 'brand_saab', 'brand_subaru',
       'brand_toyota', 'brand_volkswagen', 'brand_volvo', 'enginetype_dohcv',
       'enginetype_l', 'enginetype_ohc', 'enginetype_ohcf', 'enginetype_ohcv',
       'enginetype_rotor', 'cylindernumber_five', 'cylindernumber_four',
       'cylindernumber_six', 'cylindernumber_three', 'cylindernumber_twelve',
       'cylindernumber_two', 'fuelsystem_2bbl', 'fuelsystem_4bbl',
       'fuelsystem_idi', 'fuelsystem_mfi', 'fuelsystem_mpfi',
       'fuelsystem_spdi', 'fuelsystem_spfi',
       'price']

clean_data = data_with_dummies[cols]
clean_data

"""### Splitting the dataset into features and target"""

X = clean_data.iloc[:, :-1].values
y = clean_data.iloc[:, -1].values

# using the the train_test_split split our data into 80/20 for training data and test data

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2, random_state=50) # setting a random state to make sure our data is reproducible

"""It is important to split the data before scaling to avoid information leakage, and since we have some features that are enconded data we performed scaling on the normal features, because if we perform on the encoded features the maning will be lost"""

scaler = StandardScaler()
scaler.fit(X_train[:, :4]) # subsetting only the features we want to scale

X_train[:, :4] = scaler.transform(X_train[:, :4])
X_test[:, :4] = scaler.transform(X_test[:, :4]) # just transforming the test data

"""### Linear Regression"""

# Instantiating the LinearRegression
reg = LinearRegression()

# Fitting the data
reg.fit(X_train,y_train)

#Predicting on X_test, which is unseen
y_hat = reg.predict(X_test)

#calculating the r^2 score
lr_score = r2_score(y_test, y_hat)
lr_score



"""### Random Forest"""

# Instantiating the RandomforestRegressor
rf = RandomForestRegressor()

# Fitting the data
rf.fit(X_train, y_train)

#Predicting on X_test, which is unseen
y_pred_rf = rf.predict(X_test)

#calculating the r^2 score
rf_score = r2_score(y_test,y_pred_rf)

rf_score

"""### XGBoostRegressor"""

# Instantiating the XGBRegressor
xgb = XGBRegressor()

# Fitting the data
xgb.fit(X_train, y_train)

#Predicting on X_test, which is unseen
y_pred = xgb.predict(X_test)

#calculating the r^2 score
xgb_score = r2_score(y_test, y_pred)

xgb_score

"""Now lets compare the predicted and actual values of the target variable"""

pred_actu_diff = pd.DataFrame(y_pred, columns=["Predictions"])

pred_actu_diff.head()

pred_actu_diff["Actual Values"] = y_test
pred_actu_diff.head()

pred_actu_diff["Residual"] = pred_actu_diff["Predictions"] - pred_actu_diff["Actual Values"]

pred_actu_diff.head()

pred_actu_diff["Difference %"] = np.absolute(pred_actu_diff["Residual"] / pred_actu_diff["Actual Values"] * 100)

pd.set_option('display.float_format', lambda x: '%.2f' % x)
pred_actu_diff.sort_values(by=['Difference %']) # sorting

pred_actu_diff.describe()

"""## The END ðŸ‘¨ðŸ¿â€ðŸ’»ðŸ˜‹

This is very beautiful hope you picked up one or two from this â¤ï¸
"""